{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74821c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefdebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5313987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_swag = pd.read_csv('swag_truncated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fb2797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ending1</th>\n",
       "      <th>ending2</th>\n",
       "      <th>ending3</th>\n",
       "      <th>ending4</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spins out of his grasp.</td>\n",
       "      <td>comes down on it.</td>\n",
       "      <td>flies from his grasp.</td>\n",
       "      <td>is hurled to the surface.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>closes windows behind him.</td>\n",
       "      <td>rises 20 feet away.</td>\n",
       "      <td>looks up at him.</td>\n",
       "      <td>stops the little boy inside.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man demonstrates how to move.</td>\n",
       "      <td>goes down the slide.</td>\n",
       "      <td>throws a machine gun.</td>\n",
       "      <td>pass on a bridge.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poses watching the group.</td>\n",
       "      <td>is in the circle clapping.</td>\n",
       "      <td>dances in the sky.</td>\n",
       "      <td>plays music as people dance.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>throws a machine gun.</td>\n",
       "      <td>runs into the camera.</td>\n",
       "      <td>pass on a bridge.</td>\n",
       "      <td>woman runs into the house.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ending1                     ending2   \n",
       "0        spins out of his grasp.           comes down on it.  \\\n",
       "1     closes windows behind him.         rises 20 feet away.   \n",
       "2  man demonstrates how to move.        goes down the slide.   \n",
       "3      poses watching the group.  is in the circle clapping.   \n",
       "4          throws a machine gun.       runs into the camera.   \n",
       "\n",
       "                 ending3                       ending4  label  \n",
       "0  flies from his grasp.     is hurled to the surface.      1  \n",
       "1       looks up at him.  stops the little boy inside.      2  \n",
       "2  throws a machine gun.             pass on a bridge.      0  \n",
       "3     dances in the sky.  plays music as people dance.      3  \n",
       "4      pass on a bridge.    woman runs into the house.      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_swag.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b484000b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb37f0652d644bfb683eea40ef46e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b9d9d2720e45dfbafb4498ce82e5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7252c1660d041c9aa75c510a52341f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329289b8336845009255f438e1648343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e60aee1ef3f46f4a78006c88d562c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3053d4ece0c84d3492a8a71d81256cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e14c4cc77a84b39a3db54f7d126614a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick the best completion for this statement:\n",
      "Question: What is the capital of France?\n",
      "A) Berlin\n",
      "B) Madrid\n",
      "C) Paris\n",
      "D) Rome\n",
      "Answer: The capital of France is Berlin.\n",
      "Question: What is the\n"
     ]
    }
   ],
   "source": [
    "### Example Code\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Define the device to use (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define a function to generate an answer\n",
    "def generate_answer(prompt, model, tokenizer, max_length=256):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length, eos_token_id=50256)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.strip()\n",
    "\n",
    "# Example prompt with questions and possible answer choices\n",
    "prompt = (\n",
    "    \"Pick the best completion for this statement:\\n\"\n",
    "    \"Question: What is the capital of France?\\n\"\n",
    "    \"A) Berlin\\n\"\n",
    "    \"B) Madrid\\n\"\n",
    "    \"C) Paris\\n\"\n",
    "    \"D) Rome\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "# Generate the answer using GPT-2\n",
    "answer = generate_answer(prompt, model, tokenizer, max_length=50)\n",
    "\n",
    "# Print the generated answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611d588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Pick the best completion for this statement:\n",
      "Question: What is the capital of France?\n",
      "0: Berlin\n",
      "1: Madrid\n",
      "2: Paris\n",
      "3: Rome\n",
      "Answer:  The capital of France is Berlin.\n",
      "Question: What\n",
      "Picked Choice Index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Define the device to use (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define a function to generate an answer\n",
    "def generate_answer(prompt, model, tokenizer, max_length=256):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length, eos_token_id=50256)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.strip()\n",
    "\n",
    "# Example prompt with questions and possible answer choices\n",
    "prompt = (\n",
    "    \"Pick the best completion for this statement:\\n\"\n",
    "    \"Question: What is the capital of France?\\n\"\n",
    "    \"0: Berlin\\n\"\n",
    "    \"1: Madrid\\n\"\n",
    "    \"2: Paris\\n\"\n",
    "    \"3: Rome\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "# Generate the answer using GPT-2\n",
    "generated_text = generate_answer(prompt, model, tokenizer, max_length=50)\n",
    "\n",
    "# Extract the index of the picked choice\n",
    "def extract_choice_index(generated_text):\n",
    "    for idx in range(4):\n",
    "        if str(idx) in generated_text:\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "# Get the index of the picked choice\n",
    "choice_index = extract_choice_index(generated_text)\n",
    "\n",
    "# Print the generated text and the choice index\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(\"Picked Choice Index:\", choice_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ece452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f7fcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Pick the best completion for this statement:\n",
      "Question: What is the capital of France?\n",
      "0: Berlin\n",
      "1: Madrid\n",
      "2: Paris\n",
      "3: Rome\n",
      "Answer: \n",
      "Generated Text: Pick the best completion for this statement:\n",
      "Question: What is the capital of France?\n",
      "0: Berlin\n",
      "1: Madrid\n",
      "2: Paris\n",
      "3: Rome\n",
      "Answer:  The capital of France is Berlin.\n",
      "Question: What\n",
      "Picked Choice Index: 0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Pick the best completion for this statement:\n",
      "Question: What is the largest planet in our solar system?\n",
      "0: Earth\n",
      "1: Mars\n",
      "2: Jupiter\n",
      "3: Saturn\n",
      "Answer: \n",
      "Generated Text: Pick the best completion for this statement:\n",
      "Question: What is the largest planet in our solar system?\n",
      "0: Earth\n",
      "1: Mars\n",
      "2: Jupiter\n",
      "3: Saturn\n",
      "Answer:  Earth is the largest planet in our solar\n",
      "Picked Choice Index: 0\n",
      "==================================================\n",
      "Prompt:\n",
      " Pick the best completion for this statement:\n",
      "Question: Who wrote 'To Kill a Mockingbird'?\n",
      "0: Harper Lee\n",
      "1: Mark Twain\n",
      "2: F. Scott Fitzgerald\n",
      "3: Ernest Hemingway\n",
      "Answer: \n",
      "Generated Text: Pick the best completion for this statement:\n",
      "Question: Who wrote 'To Kill a Mockingbird'?\n",
      "0: Harper Lee\n",
      "1: Mark Twain\n",
      "2: F. Scott Fitzgerald\n",
      "3: Ernest Hemingway\n",
      "Answer:\n",
      "Picked Choice Index: 0\n",
      "==================================================\n",
      "Prompt:\n",
      " Pick the best completion for this statement:\n",
      "Question: What is the chemical symbol for gold?\n",
      "0: Au\n",
      "1: Ag\n",
      "2: Pb\n",
      "3: Fe\n",
      "Answer: \n",
      "Generated Text: Pick the best completion for this statement:\n",
      "Question: What is the chemical symbol for gold?\n",
      "0: Au\n",
      "1: Ag\n",
      "2: Pb\n",
      "3: Fe\n",
      "Answer:  The chemical symbol for gold is Au. The\n",
      "Picked Choice Index: 0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Define the device to use (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define a function to generate an answer\n",
    "def generate_answer(prompt, model, tokenizer, max_length=256):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length, eos_token_id=50256)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.strip()\n",
    "\n",
    "# Define a function to extract the index of the picked choice\n",
    "def extract_choice_index(generated_text):\n",
    "    for idx in range(4):\n",
    "        if str(idx) in generated_text:\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "# List of prompts with questions and choices\n",
    "prompts = [\n",
    "    (\n",
    "        \"Pick the best completion for this statement:\\n\"\n",
    "        \"Question: What is the capital of France?\\n\"\n",
    "        \"0: Berlin\\n\"\n",
    "        \"1: Madrid\\n\"\n",
    "        \"2: Paris\\n\"\n",
    "        \"3: Rome\\n\"\n",
    "        \"Answer: \"\n",
    "    ),\n",
    "    (\n",
    "        \"Pick the best completion for this statement:\\n\"\n",
    "        \"Question: What is the largest planet in our solar system?\\n\"\n",
    "        \"0: Earth\\n\"\n",
    "        \"1: Mars\\n\"\n",
    "        \"2: Jupiter\\n\"\n",
    "        \"3: Saturn\\n\"\n",
    "        \"Answer: \"\n",
    "    ),\n",
    "    (\n",
    "        \"Pick the best completion for this statement:\\n\"\n",
    "        \"Question: Who wrote 'To Kill a Mockingbird'?\\n\"\n",
    "        \"0: Harper Lee\\n\"\n",
    "        \"1: Mark Twain\\n\"\n",
    "        \"2: F. Scott Fitzgerald\\n\"\n",
    "        \"3: Ernest Hemingway\\n\"\n",
    "        \"Answer: \"\n",
    "    ),\n",
    "    (\n",
    "        \"Pick the best completion for this statement:\\n\"\n",
    "        \"Question: What is the chemical symbol for gold?\\n\"\n",
    "        \"0: Au\\n\"\n",
    "        \"1: Ag\\n\"\n",
    "        \"2: Pb\\n\"\n",
    "        \"3: Fe\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Generate answers and extract choice indices for each prompt\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_answer(prompt, model, tokenizer, max_length=50)\n",
    "    choice_index = extract_choice_index(generated_text)\n",
    "    print(\"Prompt:\\n\", prompt)\n",
    "    print(\"Generated Text:\", generated_text)\n",
    "    print(\"Picked Choice Index:\", choice_index)\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad76da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example Code\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Define the device to use (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define a function to generate an answer\n",
    "def generate_answer(prompt, model, tokenizer, max_length=256):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length, eos_token_id=50256, pad_token_id=50256)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.strip()\n",
    "\n",
    "# Define a function to extract the index of the picked choice\n",
    "def extract_choice_index(generated_text, prompt_length):\n",
    "    # Extract the part of the generated text that comes after the prompt\n",
    "    generated_response = generated_text[prompt_length:]\n",
    "    # Check if the generated response contains any of the choice indices\n",
    "    for idx in range(4):\n",
    "        if f\"{idx}\" in generated_response:\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "# List of prompts with questions and choices\n",
    "prompts = [\n",
    "    (\n",
    "        \"Pick the best completion for this statement:\\n\"\n",
    "        \"Question: What is the capital of France?\\n\"\n",
    "        \"0: Berlin\\n\"\n",
    "        \"1: Madrid\\n\"\n",
    "        \"2: Paris\\n\"\n",
    "        \"3: Rome\\n\"\n",
    "        \"Answer: \"\n",
    "    ),\n",
    "    (\n",
    "        \"Pick the best completion for this statement:\\n\"\n",
    "        \"Question: What is the largest planet in our solar system?\\n\"\n",
    "        \"0: Earth\\n\"\n",
    "        \"1: Mars\\n\"\n",
    "        \"2: Jupiter\\n\"\n",
    "        \"3: Saturn\\n\"\n",
    "        \"Answer: \"\n",
    "    ),\n",
    "    (\n",
    "        \"Pick the best completion for this statement:\\n\"\n",
    "        \"Question: Who wrote 'To Kill a Mockingbird'?\\n\"\n",
    "        \"0: Harper Lee\\n\"\n",
    "        \"1: Mark Twain\\n\"\n",
    "        \"2: F. Scott Fitzgerald\\n\"\n",
    "        \"3: Ernest Hemingway\\n\"\n",
    "        \"Answer: \"\n",
    "    ),\n",
    "    (\n",
    "        \"Pick the best completion for this statement:\\n\"\n",
    "        \"Question: What is the chemical symbol for gold?\\n\"\n",
    "        \"0: Au\\n\"\n",
    "        \"1: Ag\\n\"\n",
    "        \"2: Pb\\n\"\n",
    "        \"3: Fe\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Generate answers and extract choice indices for each prompt\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_answer(prompt, model, tokenizer, max_length=50)\n",
    "    choice_index = extract_choice_index(generated_text, len(prompt))\n",
    "    print(\"Prompt:\\n\", prompt)\n",
    "    print(\"Generated Text:\", generated_text)\n",
    "    print(\"Picked Choice Index:\", choice_index)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example Code\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Define the device to use (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define a function to calculate log-probabilities of a given completion\n",
    "def calculate_log_prob(prompt, completion, model, tokenizer):\n",
    "    # Combine prompt and completion\n",
    "    text = prompt + completion\n",
    "    # Tokenize the combined text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    # Calculate the model's output logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    # Get the loss (negative log-likelihood)\n",
    "    log_prob = -outputs.loss.item()\n",
    "    return log_prob\n",
    "\n",
    "# Define a function to find the best completion\n",
    "def find_best_completion(prompt, choices, model, tokenizer):\n",
    "    best_choice = None\n",
    "    best_log_prob = float('-inf')\n",
    "    \n",
    "    for choice in choices:\n",
    "        log_prob = calculate_log_prob(prompt, choice, model, tokenizer)\n",
    "        if log_prob > best_log_prob:\n",
    "            best_log_prob = log_prob\n",
    "            best_choice = choice\n",
    "            \n",
    "    return best_choice\n",
    "\n",
    "# List of prompts with choices\n",
    "prompts_with_choices = [\n",
    "    (\n",
    "        \"I am feeling nervous about my midterm tomorrow. I fear that\",\n",
    "        [\" I will not perform well.\", \" I will forget everything.\", \" I will do great.\", \" I will have fun.\"]\n",
    "    ),\n",
    "    (\n",
    "        \"I am excited about the party tonight because\",\n",
    "        [\" I will meet my friends.\", \" the food will be amazing.\", \" I will get to dance all night.\", \" I will have a great time.\"]\n",
    "    ),\n",
    "    (\n",
    "        \"The new policy implemented by the government is expected to\",\n",
    "        [\" improve the economy.\", \" cause a recession.\", \" have no effect.\", \" increase inflation.\"]\n",
    "    ),\n",
    "    (\n",
    "        \"The quick brown fox jumps over the\",\n",
    "        [\" lazy dog.\", \" tall fence.\", \" sleeping cat.\", \" running deer.\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Find and print the best completion for each prompt\n",
    "for prompt, choices in prompts_with_choices:\n",
    "    best_completion = find_best_completion(prompt, choices, model, tokenizer)\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Best Completion:\", best_completion)\n",
    "    print(\"=\"*50)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Define the Device**: Determine if a GPU is available; if so, use it. Otherwise, use the CPU.\n",
    "2. **Load the Model and Tokenizer**: Load the pre-trained GPT-2 model and tokenizer from the `transformers` library.\n",
    "3. **Define the `calculate_log_prob` Function**:\n",
    "   - **Combine Prompt and Completion**: Concatenate the prompt and the completion.\n",
    "   - **Tokenize**: Tokenize the combined text and encode it into token IDs.\n",
    "   - **Calculate Log-Probabilities**: Use the model to calculate the log-probabilities of the completion given the prompt.\n",
    "4. **Define the `find_best_completion` Function**:\n",
    "   - **Initialize Best Choice and Log Probability**: Start with the lowest possible log-probability.\n",
    "   - **Iterate Over Choices**: Calculate the log-probabilities for each choice and select the one with the highest log-probability.\n",
    "5. **List of Prompts with Choices**: Create a list of prompts, each with a corresponding list of choices.\n",
    "6. **Find and Print the Best Completion for Each Prompt**: Iterate through each prompt and find the best completion using the `find_best_completion` function. Print the results.\n",
    "\n",
    "### Output\n",
    "\n",
    "```plaintext\n",
    "Prompt: I am feeling nervous about my midterm tomorrow. I fear that\n",
    "Best Completion: I will do great.\n",
    "==================================================\n",
    "Prompt: I am excited about the party tonight because\n",
    "Best Completion: I will have a great time.\n",
    "==================================================\n",
    "Prompt: The new policy implemented by the government is expected to\n",
    "Best Completion: improve the economy.\n",
    "==================================================\n",
    "Prompt: The quick brown fox jumps over the\n",
    "Best Completion: lazy dog.\n",
    "==================================================\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
